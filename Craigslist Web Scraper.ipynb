{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ea2e9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 170 listings\n"
     ]
    }
   ],
   "source": [
    "#Import necessary libraries\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from collections import namedtuple\n",
    "import selenium.webdriver as webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the user agent and chrome driver path\n",
    "user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36'\n",
    "chrome_driver_path = os.path.join(os.getcwd(), 'chromedriver.exe')\n",
    "\n",
    "# Set up the chrome service and options\n",
    "chrome_service = Service(chrome_driver_path)\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(f'user-agent={user_agent}')\n",
    "\n",
    "# Create a new webdriver instance\n",
    "browser = webdriver.Chrome(service=chrome_service, options=chrome_options)\n",
    "browser.implicitly_wait(7)\n",
    "\n",
    "# Set the URL to scrape\n",
    "url = 'https://atlanta.craigslist.org/'\n",
    "browser.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "# Define the Output Excel File name \n",
    "search_query = 'Laptop_Craigslist'\n",
    "\n",
    "# Click the \"For Sale\" hyperlink\n",
    "for_sale_element = browser.find_element(By.XPATH, \"//a[@data-alltitle='all for sale']\")\n",
    "for_sale_element.click()\n",
    "time.sleep(2)\n",
    "\n",
    "# Select \"All Atlanta\" from the neighborhood dropdown menu\n",
    "dropdown_neighborhood = browser.find_element(\"xpath\", \"//*[@class='cl-breadcrumb subarea-selector bd-combo-box static bd-vStat-valid']\")\n",
    "dropdown_neighborhood.click()\n",
    "select_neighrborhood = browser.find_element(\"xpath\", \"//*[@class='bd-button bd-list-box-focused-item']\")\n",
    "select_neighrborhood.click()\n",
    "time.sleep(2)\n",
    "\n",
    "# Select \"Computer\" from the item category  dropdown menu\n",
    "dropdown_item_catagory = browser.find_element(\"xpath\", \"//*[@class='cl-breadcrumb category-selector bd-combo-box static bd-vStat-valid']\")\n",
    "dropdown_item_catagory.click()\n",
    "select_item_catagory = browser.find_element(\"xpath\", \"//*[@class='bd-button sya']\")\n",
    "select_item_catagory.click()\n",
    "time.sleep(2)\n",
    "\n",
    "# Find the search input element by its attributes\n",
    "search_input = browser.find_element(\"xpath\", \"//input [@placeholder='search computers']\")\n",
    "\n",
    "# Search \"Laptop\" into the search input element\n",
    "search_input.send_keys(\"laptop\")\n",
    "\n",
    "# Press the \"Enter\" key to submit the search query\n",
    "search_input.send_keys(Keys.ENTER)\n",
    "time.sleep(0.5)\n",
    "\n",
    "# Create a list to store the scraped data\n",
    "posts_html = []\n",
    "\n",
    "# start a loop to navigate through pages\n",
    "to_stop = False\n",
    "while not to_stop:\n",
    "    \n",
    "    #Locate the Search Results List and parse the HTML\n",
    "    search_results = browser.find_element(By.ID, 'search-results-page-1')\n",
    "    soup = BeautifulSoup(search_results.get_attribute('innerHTML'), 'html.parser') \n",
    "    posts_html.extend(soup.find_all('li', {'class': 'cl-search-result cl-search-view-mode-gallery'}))\n",
    "    \n",
    "    #scroll the page to the top and click \"Next Page\" Button\n",
    "    try:\n",
    "        browser.execute_script('window.scrollTo(0, 0)')\n",
    "        button_next = browser.find_element(By.XPATH, \"//*[@class='bd-button cl-next-page icon-only']\")\n",
    "        button_next.click()\n",
    "        time.sleep(0.5)\n",
    "    except NoSuchElementException:\n",
    "        to_stop = True\n",
    "\n",
    "# Print the number of listings collected\n",
    "print('Collected {0} listings'.format(len(posts_html)))\n",
    "    \n",
    "\n",
    "# Define the fields for each Craigslist post and initialize a list to store the posts\n",
    "CraigslistPost = namedtuple('CraigslistPost', ['Title', 'Price','Date', 'Location', 'Post_url', 'Image_url'])\n",
    "craigslist_posts = []\n",
    "\n",
    "\n",
    "\n",
    "for post_html in posts_html:\n",
    "    \n",
    "    #Look for the \"title\"\n",
    "    title_text = post_html.find('a', {'class': 'titlestring'})\n",
    "    if title_text:\n",
    "        title = title_text.text\n",
    "    else:\n",
    "        title = None\n",
    "        \n",
    "    #Look for the \"price\"\n",
    "    price_text = post_html.find('span', {'class': 'priceinfo'})\n",
    "    if price_text:\n",
    "        price = price_text.text\n",
    "    else:\n",
    "        price = None\n",
    "        \n",
    "    #Look for the \"date\" and \"location\"\n",
    "    meta_text = post_html.find('div', {'class': 'meta'})\n",
    "    if meta_text:\n",
    "        \n",
    "        #Split the meta text using the separator\n",
    "        parts = meta_text.text.strip().split('Â·')\n",
    "        date = parts[0].strip()\n",
    "        location = parts[1].strip()\n",
    "    else:\n",
    "        date = None\n",
    "        location = None\n",
    "        \n",
    "    #Look for the \"post url\"\n",
    "    post_url = post_html.find('a', 'titlestring').get('href')\n",
    "    \n",
    "    #Look for the \"image url\"\n",
    "    image_url = post_html.find('img').get('src') if post_html.find('img') else ''\n",
    "    \n",
    "    #Append the List\n",
    "    craigslist_posts.append(CraigslistPost(title, price, date, location, post_url, image_url))\n",
    "\n",
    "\n",
    "# Convert the named tuple list to a Pandas DataFrame and save it to an Excel file\n",
    "df = pd.DataFrame(craigslist_posts)\n",
    "current_time = datetime.datetime.now().strftime(\"%m_%d_%Y %I_%M %p\")\n",
    "file_name = f'{search_query} ({current_time}).xlsx'\n",
    "df.to_excel(file_name, index=False)\n",
    "\n",
    "# Close the webdriver\n",
    "browser.close()\n",
    "\n",
    "# Open the file in Microsoft Excel\n",
    "if os.name == 'nt':\n",
    "    os.startfile(file_name, 'open')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32e4ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fa9dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
